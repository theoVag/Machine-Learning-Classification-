{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_diabetes()\n",
    "data_x = dataset.data\n",
    "print(data_x.shape)\n",
    "data_y = dataset.target\n",
    "print(data_y.shape)\n",
    "print(\"Number of nan's in dataset:\",np.sum(np.isnan(data_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot each pair (column,target)\n",
    "for i in range(data_x.shape[1]):\n",
    "    print(\"Feature %d\" % (i))\n",
    "    plt.plot(data_x[:,i],data_y,'x')\n",
    "    #plt.xlabel(data_x.columns[i])\n",
    "    plt.ylabel(\"Target\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=0,shuffle=True)\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression OLS\n",
    "lr=lm.LinearRegression()\n",
    "mse_train_ols =cross_val_score(lr, X_train, y_train, cv=10,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean OLS Train MSE: %f (%f)\" %(-np.mean(mse_train_ols),np.std(mse_train_ols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge tune alpha\n",
    "clf = RidgeCV(cv=10,alphas=[1e-3, 1e-2, 1e-1,1e0, 1e1,1e2,1e3]).fit(X_train_norm, y_train)\n",
    "print(\"Ridge alpha=\",clf.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "ridge = lm.Ridge(alpha=1.0,normalize=False)\n",
    "mse_train_ridge =cross_val_score(ridge, X_train_norm, y_train, cv=10,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean Ridge Train MSE: %f (%f)\" %(-np.mean(mse_train_ridge),np.std(mse_train_ridge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logr = lm.LogisticRegression(penalty='l2' ,C =100.0, multi_class = 'multinomial', solver = 'lbfgs', max_iter = 10000,tol = 1e-3, verbose=1)\n",
    "mse_train_logr =cross_val_score(logr, X_train_norm, y_train, cv=5,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean Logistic Regression Train MSE\",-np.mean(mse_train_logr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNE SVR\n",
    "Cs = [2**(-3),2**(-2),2**(-1), 1,2**(1),2**(2),2**(3),2**(4),2**(5),2**(6),2**(7),2**(8),2**(9),2**(10),2**(11),2**(12),2**(13),2**(14),2**(15)]\n",
    "eps = [2**(-15),2**(-14),2**(-13),2**(-12),2**(-11),2**(-10),2**(-9),2**(-8),2**(-7),2**(-6),2**(-5),2**(-4),2**(-3),2**(-2),2**(-1),2**(0),2**(1),2**(2),2**(3)]\n",
    "\n",
    "parameters = {'C':Cs,'epsilon':eps}\n",
    "svc = svm.SVR( kernel='rbf',gamma='scale')\n",
    "clf = GridSearchCV(svc, parameters, cv=5,verbose=2,n_jobs=-1,scoring = 'neg_mean_squared_error')\n",
    "clf.fit(X_train_norm, y_train)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (clf.best_params_, clf.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "clf2 = svm.SVR(C=64, epsilon=6*1e-5,gamma='scale')\n",
    "clf2.fit(X_train_norm, y_train)\n",
    "y_hat = clf2.predict(X_test_norm)\n",
    "mse_svr=np.mean((y_test-y_hat)**2)\n",
    "mse_svr\n",
    "mse_train_svm =cross_val_score(clf2, X_train_norm, y_train, cv=10,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean SVR Train MSE: %f (%f)\",(-np.mean(mse_train_svm),np.std(mse_train_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests tune\n",
    "min_samples_split=[0.1*x for x in range(1,6)]\n",
    "max_depth=[None]\n",
    "max_features=[x for x in range(1,11)]\n",
    "min_samples_leaf=[0.1*x for x in range(1,6)]\n",
    "n_estimators=[50]\n",
    "parameters = {'n_estimators':n_estimators,'max_depth':max_depth,'min_samples_split':min_samples_split,'max_features':max_features,'min_samples_leaf':min_samples_leaf}\n",
    "rrforest = ensemble.RandomForestRegressor(criterion='mse',\n",
    "                                          min_weight_fraction_leaf=0.0,\n",
    "                                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                          min_impurity_split=None, bootstrap=True, oob_score=True,\n",
    "                                          n_jobs=1, random_state=0, verbose=0,\n",
    "                                          warm_start=False)\n",
    "\n",
    "clf = GridSearchCV(rrforest, parameters, cv=10,scoring='neg_mean_squared_error',verbose=2,n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (clf.best_params_, clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "rrforest = ensemble.RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=None,\n",
    "                                          min_samples_split=0.1, min_samples_leaf=0.1,\n",
    "                                          min_weight_fraction_leaf=0.0, max_features=6,\n",
    "                                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                          min_impurity_split=None, bootstrap=True, oob_score=True,\n",
    "                                          n_jobs=1, random_state=0, verbose=0,\n",
    "                                          warm_start=False)\n",
    "rrforest_scores = -cross_val_score(rrforest, X_train, y_train, cv=10,\n",
    "                                   scoring='neg_mean_squared_error')\n",
    "print(\"Random forests on dataset, estimated MSE: %f (%f)\" % (np.mean(rrforest_scores), np.std(rrforest_scores)))\n",
    "\n",
    "rrforest.fit(X_train,y_train)\n",
    "y_hat = rrforest.predict(X_test)\n",
    "mse_rf=np.mean((y_test-y_hat)**2)\n",
    "print(\"Random Forests TEST MSE:\",mse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparisons with results in test\n",
    "#Define function for MAPE calculation\n",
    "def mape(y_true,y_pred):\n",
    "\ty_true,y_pred = np.array(y_true), np.array(y_pred)\n",
    "\treturn np.mean(np.abs((y_true-y_pred)/y_true))\n",
    "\n",
    "# OLS\n",
    "lr=lm.LinearRegression().fit(X_train,y_train)\n",
    "yhat=lr.predict(X_test)\n",
    "mse_ols=np.mean((y_test-yhat)**2)\n",
    "mae_ols=np.mean(np.fabs(y_test-yhat))\n",
    "my_mape=mape(y_test,yhat)\n",
    "print('MSE TEST OLS: %f' % mse_ols)\n",
    "#print('MAE OLS: %f' % mae_ols)\n",
    "#print(\"MAPE OLS: %f\" % my_mape)\n",
    "\n",
    "#RIDGE \n",
    "ridge = lm.Ridge(alpha=1.0,normalize=False)\n",
    "ridge.fit(X_train_norm,y_train)\n",
    "yhat=ridge.predict(X_test_norm)\n",
    "mse_ridge=np.mean((y_test-yhat)**2)\n",
    "mae_ridge=np.mean(np.fabs(y_test-yhat))\n",
    "my_mape=mape(y_test,yhat)\n",
    "print('MSE TEST RIDGE: %f' % (mse_ridge))\n",
    "\n",
    "#RANDOM FORESTS\n",
    "rrforest.fit(X_train,y_train)\n",
    "y_hat=rrforest.predict(X_test)\n",
    "mse_rrf=np.mean((y_test-y_hat)**2)\n",
    "print(\"MSE TEST RAND FOR\",mse_rrf)\n",
    "\n",
    "#SVR\n",
    "clf2 = svm.SVR(C=64.0, epsilon=6*1e-5,gamma='scale')\n",
    "clf2.fit(X_train_norm, y_train)\n",
    "y_hat = clf2.predict(X_test_norm)\n",
    "mse_svr=np.mean((y_test-y_hat)**2)\n",
    "print(\"MSE TEST SVR\",mse_svr)\n",
    "print(\"MAPE TEST SVR\",mape(y_test,y_hat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR CHOSEN\n",
    "# FIT AND PREPARE FOR USE\n",
    "# MY ESTIMATION FOR MSE IS 3376\n",
    "clf2.fit(data_x,data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
