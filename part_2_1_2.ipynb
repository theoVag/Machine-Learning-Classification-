{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LassoCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_diabetes()\n",
    "data_x = dataset.data\n",
    "data_y = dataset.target\n",
    "print(\"Number of nan's in dataset:\",np.sum(np.isnan(data_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_y[:300,]\n",
    "y_test = data_y[300:,]\n",
    "\n",
    "X_train = data_x[:300,:(data_x.shape[1])]\n",
    "X_test= data_x[300:,:(data_x.shape[1])]\n",
    "# Normalized to [0,1]\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS\n",
    "# Train mse with cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "lr=lm.LinearRegression()\n",
    "mse_train_ols =cross_val_score(lr, X_train, y_train, cv=10,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean OLS Train MSE: %f (%f)\" %(-np.mean(mse_train_ols),np.std(mse_train_ols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge tune alpha\n",
    "clf = RidgeCV(cv=10,alphas=[1e-3, 1e-2, 1e-1,1e0, 1e1,1e2,1e3]).fit(X_train_norm, y_train)\n",
    "print(\"Ridge alpha=\",clf.alpha_)\n",
    "# Ridge regression\n",
    "ridge = lm.Ridge(alpha=clf.alpha_,normalize=False)\n",
    "mse_train_ridge =cross_val_score(ridge, X_train_norm, y_train, cv=10,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean Ridge Train MSE: %f (%f)\" %(-np.mean(mse_train_ridge),np.std(mse_train_ridge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Lasso alpha\n",
    "reg = LassoCV(alphas=[1e-3, 1e-2, 1e-1,1e0, 1e1,1e2,1e3],cv=5).fit(X_train_norm, y_train)\n",
    "print(\"Alpha for Lasso:\",reg.alpha_)\n",
    "# Lasso regression\n",
    "lasso=lm.Lasso(reg.alpha_)\n",
    "lasso.fit(X_train_norm,y_train)\n",
    "mse_train_lasso =cross_val_score(lasso, X_train_norm, y_train, cv=10,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean Lasso Train MSE: %f (%f)\" %(-np.mean(mse_train_lasso),np.std(mse_train_lasso)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logr = lm.LogisticRegression(penalty='l2' ,C =10.0, multi_class = 'multinomial', solver = 'lbfgs', max_iter = 10000,tol = 1e-3, verbose=1)\n",
    "logr.fit(X_train_norm,y_train)\n",
    "\n",
    "mse_train_logr =cross_val_score(logr, X_train_norm, y_train, cv=5,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean Logistic Regression Train MSE: %f (%f)\" %(-np.mean(mse_train_logr),np.std(mse_train_logr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune SVR parameters\n",
    "Cs = [2**(-3),2**(-2),2**(-1), 1,2**(1),2**(2),2**(3),2**(4),2**(5),2**(6),2**(7),2**(8),2**(9),2**(10),2**(11),2**(12),2**(13),2**(14),2**(15)]\n",
    "eps = [2**(-15),2**(-14),2**(-13),2**(-12),2**(-11),2**(-10),2**(-9),2**(-8),2**(-7),2**(-6),2**(-5),2**(-4),2**(-3),2**(-2),2**(-1),2**(0),2**(1),2**(2),2**(3)]\n",
    "\n",
    "cv=StratifiedKFold(n_splits=5).split(X_train_norm, y_train)\n",
    "\n",
    "parameters = {'C':Cs,'epsilon':eps}\n",
    "svc = svm.SVR( kernel='rbf',gamma='scale')\n",
    "clf = GridSearchCV(svc, parameters, cv=cv,verbose=2,n_jobs=-1,scoring = 'neg_mean_squared_error')\n",
    "clf.fit(X_train_norm, y_train)\n",
    "print(\"The best parameters are %s with a score of %f\"% (clf.best_params_, clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "svr_model = svm.SVR(C=128.0, epsilon=8,gamma='scale')\n",
    "svr_model.fit(X_train_norm, y_train)\n",
    "mse_train_svm =cross_val_score(svr_model, X_train_norm, y_train, cv=10,scoring = 'neg_mean_squared_error')\n",
    "print(\"Mean SVR Train MSE: %f (%f)\",((-np.mean(mse_train_svm)),np.std(mse_train_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor tune parameters\n",
    "min_samples_split=[0.1*x for x in range(1,11)]\n",
    "max_depth=[x for x in range(1,31)]\n",
    "max_features=[x for x in range(1,11)]\n",
    "min_samples_leaf=[0.1*x for x in range(1,6)]\n",
    "parameters = {'max_depth':max_depth,'min_samples_split':min_samples_split,'max_features':max_features,'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "rtree = tree.DecisionTreeRegressor(criterion='mse', splitter='best',                                    \n",
    "                                   min_weight_fraction_leaf=0.0,\n",
    "                                   random_state=0, max_leaf_nodes=None,\n",
    "                                   min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                   presort=False)\n",
    "\n",
    "clf = GridSearchCV(rtree, parameters, cv=10,scoring='neg_mean_squared_error',verbose=2,n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The best parameters are %s with a score of %f\" % (clf.best_params_, clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor\n",
    "rtree = tree.DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=4,\n",
    "                                   min_samples_split=0.1, min_samples_leaf=0.1,\n",
    "                                   min_weight_fraction_leaf=0.0, max_features=9,\n",
    "                                   random_state=0, max_leaf_nodes=None,\n",
    "                                   min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                   presort=False)\n",
    "rtree_scores = -cross_val_score(rtree, X_train, y_train, cv=5,\n",
    "                                scoring='neg_mean_squared_error')\n",
    "\n",
    "rtree.fit(X_train,y_train)\n",
    "\n",
    "print(\"Regression trees on train, estimated MSE: %f (%f)\" %(np.mean(rtree_scores), np.std(rtree_scores)))\n",
    "\n",
    "#y_hat = rtree.predict(X_test)\n",
    "#mse_rtree=np.mean((y_test-y_hat)**2)\n",
    "#print(\"DecisionTreeRegressor TEST MSE:\",mse_rtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests tune parameters\n",
    "min_samples_split=[0.1*x for x in range(1,11)]\n",
    "max_depth=[x for x in range(1,31)]\n",
    "max_features=[x for x in range(1,11)]\n",
    "min_samples_leaf=[1]\n",
    "n_estimators=[x for x in range(1,100,10)]\n",
    "parameters = {'n_estimators':n_estimators,'max_depth':max_depth,'min_samples_split':min_samples_split,'max_features':max_features,'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "rrforest = ensemble.RandomForestRegressor(criterion='mse',\n",
    "                                          min_weight_fraction_leaf=0.0,\n",
    "                                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                          min_impurity_split=None, bootstrap=True, oob_score=True,\n",
    "                                          n_jobs=1, random_state=0, verbose=0,\n",
    "                                          warm_start=False)\n",
    "\n",
    "clf = GridSearchCV(rrforest, parameters, cv=10,scoring='neg_mean_squared_error',verbose=2,n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The best parameters are %s with a score of %f\" % (clf.best_params_, clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "#max_features = int(np.ceil(np.sqrt(X_train.shape[1]) / 3.0))\n",
    "rrforest = ensemble.RandomForestRegressor(n_estimators=30, criterion='mse', max_depth=4,\n",
    "                                          min_samples_split=0.1, min_samples_leaf=1,\n",
    "                                          min_weight_fraction_leaf=0.0, max_features=6,\n",
    "                                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                          min_impurity_split=None, bootstrap=True, oob_score=False,\n",
    "                                          n_jobs=1, random_state=None, verbose=0,\n",
    "                                          warm_start=False)\n",
    "rrforest_scores = -cross_val_score(rrforest, X_train, y_train, cv=5,\n",
    "                                   scoring='neg_mean_squared_error')\n",
    "print(\"Random forests on train, estimated MSE: %f (%f)\" % (np.mean(rrforest_scores), np.std(rrforest_scores)))\n",
    "rrforest.fit(X_train,y_train)\n",
    "#y_hat = rrforest.predict(X_test)\n",
    "#mse_rf=np.mean((y_test-y_hat)**2)\n",
    "#print(\"Random Forests TEST MSE:\",mse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE CHOSEN MODEL IN TEST\n",
    "y_hat = svr_model.predict(X_test_norm)\n",
    "mse_svr=np.mean((y_test-y_hat)**2)\n",
    "print(\"SVR TEST MSE:\",mse_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
